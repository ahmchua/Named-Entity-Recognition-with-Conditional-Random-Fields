{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models.py\n",
    "\n",
    "from nerdata import *\n",
    "from utils import *\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scoring function for sequence models based on conditional probabilities.\n",
    "# Scores are provided for three potentials in the model: initial scores (applied to the first tag),\n",
    "# emissions, and transitions. Note that CRFs typically don't use potentials of the first type.\n",
    "class ProbabilisticSequenceScorer(object):\n",
    "    def __init__(self, tag_indexer, word_indexer, init_log_probs, transition_log_probs, emission_log_probs):\n",
    "        self.tag_indexer = tag_indexer\n",
    "        self.word_indexer = word_indexer\n",
    "        self.init_log_probs = init_log_probs\n",
    "        self.transition_log_probs = transition_log_probs\n",
    "        self.emission_log_probs = emission_log_probs\n",
    "\n",
    "    def score_init(self, sentence, tag_idx):\n",
    "        return self.init_log_probs[tag_idx]\n",
    "\n",
    "    def score_transition(self, sentence, prev_tag_idx, curr_tag_idx):\n",
    "        return self.transition_log_probs[prev_tag_idx, curr_tag_idx]\n",
    "\n",
    "    def score_emission(self, sentence, tag_idx, word_posn):\n",
    "        word = sentence.tokens[word_posn].word\n",
    "        word_idx = self.word_indexer.index_of(word) if self.word_indexer.contains(word) else self.word_indexer.get_index(\"UNK\")\n",
    "        return self.emission_log_probs[tag_idx, word_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HmmNerModel(object):\n",
    "    def __init__(self, tag_indexer, word_indexer, init_log_probs, transition_log_probs, emission_log_probs):\n",
    "        self.tag_indexer = tag_indexer\n",
    "        self.word_indexer = word_indexer\n",
    "        self.init_log_probs = init_log_probs\n",
    "        self.transition_log_probs = transition_log_probs\n",
    "        self.emission_log_probs = emission_log_probs\n",
    "\n",
    "    # Takes a LabeledSentence object and returns a new copy of that sentence with a set of chunks predicted by\n",
    "    # the HMM model. See BadNerModel for an example implementation\n",
    "    def decode(self, sentence):\n",
    "        raise Exception(\"IMPLEMENT ME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T is the number of observations, N is the number of states\n",
    "def viterbi (T,N, init_counts, emission_counts, transition_counts):\n",
    "    path_prob_mat = np.ones((N,T), dtype=float) * 0.00\n",
    "    backpointer = np.ones((N,T), dtype=int) * 0\n",
    "\n",
    "# Initial counts and emission counts are already stored as logs\n",
    "# Note that log(xy) = log(x) + log(y)\n",
    "    for state in range(N):\n",
    "        path_prob_mat[state, 1] = init_counts[state] + emission_counts[state,1]\n",
    "        backpointer[state,1] = 0\n",
    "\n",
    "    for t in range(1,T):\n",
    "        for s in range(0,N):\n",
    "            probabilities = path_prob_mat[0:N,t-1] + transition_counts[0:N,s] +  emission_counts[s,t]\n",
    "            path_prob_mat[s,t] = probabilities.max()\n",
    "            backpointer[s,t] = probabilities.argmax()\n",
    "\n",
    "    best_path_prob = np.exp(path_prob_mat[0:N,T-1].max())\n",
    "    best_path_pointer = path_prob_mat[0:N,T-1].argmax()\n",
    "    best_path = []\n",
    "    s = best_path_pointer\n",
    "    for i in range(T-1,-1,-1):\n",
    "        best_path.append(s)\n",
    "        s = backpointer[s,i]\n",
    "        #best_path.append(s)\n",
    "    \n",
    "    print(backpointer)\n",
    "    print(\"The best path pointer is\",best_path_pointer)\n",
    "    print(\"The path probability matrix is : \", np.exp(path_prob_mat))\n",
    "    return best_path, np.exp(best_path_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emission counts after smoothing [[0.001032 0.001    0.001    0.001048 0.001   ]\n",
      " [0.001    0.309431 0.001    0.001    0.001   ]\n",
      " [0.001    0.001028 0.001672 0.001    0.001028]\n",
      " [0.001    0.001    0.00134  0.001    0.001   ]\n",
      " [0.001    0.0012   0.001223 0.001    0.003337]\n",
      " [0.001    0.001    0.011446 0.001    0.001   ]\n",
      " [0.001    0.001    0.001    0.507099 0.001   ]]\n",
      "Sum of counts is  0.863884\n",
      "[[0 0 0 2 6]\n",
      " [0 4 4 5 6]\n",
      " [0 1 1 5 6]\n",
      " [0 6 2 5 6]\n",
      " [0 6 6 2 6]\n",
      " [0 1 1 5 6]\n",
      " [0 2 2 2 6]]\n",
      "The best path pointer is 4\n",
      "The path probability matrix is :  [[1.00000000e+00 4.37211477e-04 1.91153876e-07 3.79763263e-10\n",
      "  1.69042578e-10]\n",
      " [1.00000000e+00 6.30407045e-03 1.34254154e-08 1.67456418e-10\n",
      "  3.09493822e-12]\n",
      " [1.00000000e+00 9.48171745e-04 9.72190171e-06 1.65978861e-09\n",
      "  3.03009190e-13]\n",
      " [1.00000000e+00 2.49686300e-04 1.23101072e-07 1.66143034e-09\n",
      "  3.17894369e-10]\n",
      " [1.00000000e+00 6.58977363e-04 1.73444226e-07 6.92103286e-10\n",
      "  2.33310120e-09]\n",
      " [1.00000000e+00 1.96554167e-04 1.41826392e-05 1.19517914e-09\n",
      "  1.50325571e-11]\n",
      " [1.00000000e+00 2.58252265e-04 2.44867501e-07 1.27317505e-06\n",
      "  2.50542618e-12]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([4, 6, 2, 1, 4], 1.0000000023331013)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_counts = np.array([0.2767, 0.0006, 0.0031, 0.0453, 0.0449, 0.0510, 0.2026])\n",
    "transition_counts = np.array([[0.3777,0.0110,0.0009,0.0084,0.0584, 0.0090,0.0025],\n",
    "                              [0.0008,0.0002,0.7968,0.0005,0.0008,0.1698,0.0041],\n",
    "                              [0.0322,0.0005,0.0050,0.0837,0.0615,0.0514,0.2231],\n",
    "                              [0.0366,0.0004,0.0001,0.0733,0.4509,0.0036,0.0036],\n",
    "                              [0.0096,0.0176,0.0014,0.0086,0.1216,0.0177,0.0068],\n",
    "                              [0.0068,0.0102,0.1011,0.1012,0.0120,0.0728,0.0479],\n",
    "                              [0.1147,0.0021,0.0002,0.2157,0.4744,0.0102,0.0017]])\n",
    "emission_counts = np.array([[0.000032,0,0,0.000048,0],\n",
    "                           [0,0.308431,0,0,0],\n",
    "                           [0,0.000028,0.000672,0,0.000028],\n",
    "                           [0,0,0.000340,0,0],\n",
    "                           [0,0.000200,0.000223,0,0.002337],\n",
    "                           [0,0,0.010446,0,0],\n",
    "                           [0,0,0,0.506099,0]])\n",
    "\n",
    "size_e0 = np.shape(emission_counts)[0]\n",
    "size_e1 = np.shape(emission_counts)[1]\n",
    "smooth = np.ones((size_e0, size_e1), dtype=float) * 0.001\n",
    "emission_counts = emission_counts + smooth\n",
    "print(\"emission counts after smoothing\", emission_counts)\n",
    "sum_counts = np.sum(emission_counts)\n",
    "print(\"Sum of counts is \",sum_counts)\n",
    "emission_counts = emission_counts/sum_counts\n",
    "transition_counts = np.log(transition_counts)\n",
    "emission_counts = np.log(emission_counts)\n",
    "init_counts = np.log(init_counts)\n",
    "viterbi(5,7,init_counts, emission_counts, transition_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieves a word's index based on its count. If the word occurs only once, treat it as an \"UNK\" token\n",
    "# At test time, unknown words will be replaced by UNKs.\n",
    "def get_word_index(word_indexer, word_counter, word):\n",
    "    if word_counter.get_count(word) < 1.5:\n",
    "        return word_indexer.get_index(\"UNK\")\n",
    "    else:\n",
    "        return word_indexer.get_index(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrfNerModel(object):\n",
    "    def __init__(self, tag_indexer, feature_indexer, feature_weights):\n",
    "        self.tag_indexer = tag_indexer\n",
    "        self.feature_indexer = feature_indexer\n",
    "        self.feature_weights = feature_weights\n",
    "\n",
    "    # Takes a LabeledSentence object and returns a new copy of that sentence with a set of chunks predicted by\n",
    "    # the CRF model. See BadNerModel for an example implementation\n",
    "    def decode(self, sentence):\n",
    "        raise Exception(\"IMPLEMENT ME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trains a CrfNerModel on the given corpus of sentences.\n",
    "def train_crf_model(sentences):\n",
    "    tag_indexer = Indexer()\n",
    "    for sentence in sentences:\n",
    "        for tag in sentence.get_bio_tags():\n",
    "            tag_indexer.get_index(tag)\n",
    "    print(\"Extracting features\")\n",
    "    feature_indexer = Indexer()\n",
    "    # 4-d list indexed by sentence index, word index, tag index, feature index\n",
    "    feature_cache = [[[[] for k in range(0, len(tag_indexer))] for j in range(0, len(sentences[i]))] for i in range(0, len(sentences))]\n",
    "    for sentence_idx in range(0, len(sentences)):\n",
    "        if sentence_idx % 100 == 0:\n",
    "            print(\"Ex %i/%i\" % (sentence_idx, len(sentences)))\n",
    "        for word_idx in range(0, len(sentences[sentence_idx])):\n",
    "            for tag_idx in range(0, len(tag_indexer)):\n",
    "                feature_cache[sentence_idx][word_idx][tag_idx] = extract_emission_features(sentences[sentence_idx], word_idx, tag_indexer.get_object(tag_idx), feature_indexer, add_to_indexer=True)\n",
    "    print(\"Training\")\n",
    "    raise Exception(\"IMPLEMENT THE REST OF ME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts emission features for tagging the word at word_index with tag.\n",
    "# add_to_indexer is a boolean variable indicating whether we should be expanding the indexer or not:\n",
    "# this should be True at train time (since we want to learn weights for all features) and False at\n",
    "# test time (to avoid creating any features we don't have weights for).\n",
    "def extract_emission_features(sentence, word_index, tag, feature_indexer, add_to_indexer):\n",
    "    feats = []\n",
    "    curr_word = sentence.tokens[word_index].word\n",
    "    # Lexical and POS features on this word, the previous, and the next (Word-1, Word0, Word1)\n",
    "    for idx_offset in range(-1, 2):\n",
    "        if word_index + idx_offset < 0:\n",
    "            active_word = \"<s>\"\n",
    "        elif word_index + idx_offset >= len(sentence):\n",
    "            active_word = \"</s>\"\n",
    "        else:\n",
    "            active_word = sentence.tokens[word_index + idx_offset].word\n",
    "        if word_index + idx_offset < 0:\n",
    "            active_pos = \"<S>\"\n",
    "        elif word_index + idx_offset >= len(sentence):\n",
    "            active_pos = \"</S>\"\n",
    "        else:\n",
    "            active_pos = sentence.tokens[word_index + idx_offset].pos\n",
    "        maybe_add_feature(feats, feature_indexer, add_to_indexer, tag + \":Word\" + repr(idx_offset) + \"=\" + active_word)\n",
    "        maybe_add_feature(feats, feature_indexer, add_to_indexer, tag + \":Pos\" + repr(idx_offset) + \"=\" + active_pos)\n",
    "    # Character n-grams of the current word\n",
    "    max_ngram_size = 3\n",
    "    for ngram_size in range(1, max_ngram_size+1):\n",
    "        start_ngram = curr_word[0:min(ngram_size, len(curr_word))]\n",
    "        maybe_add_feature(feats, feature_indexer, add_to_indexer, tag + \":StartNgram=\" + start_ngram)\n",
    "        end_ngram = curr_word[max(0, len(curr_word) - ngram_size):]\n",
    "        maybe_add_feature(feats, feature_indexer, add_to_indexer, tag + \":EndNgram=\" + end_ngram)\n",
    "    # Look at a few word shape features\n",
    "    maybe_add_feature(feats, feature_indexer, add_to_indexer, tag + \":IsCap=\" + repr(curr_word[0].isupper()))\n",
    "    # Compute word shape\n",
    "    new_word = []\n",
    "    for i in range(0, len(curr_word)):\n",
    "        if curr_word[i].isupper():\n",
    "            new_word += \"X\"\n",
    "        elif curr_word[i].islower():\n",
    "            new_word += \"x\"\n",
    "        elif curr_word[i].isdigit():\n",
    "            new_word += \"0\"\n",
    "        else:\n",
    "            new_word += \"?\"\n",
    "    maybe_add_feature(feats, feature_indexer, add_to_indexer, tag + \":WordShape=\" + repr(new_word))\n",
    "    return np.asarray(feats, dtype=int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
